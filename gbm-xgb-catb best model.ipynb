{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91723,"databundleVersionId":14272474,"sourceType":"competition"}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T21:00:05.624315Z","iopub.execute_input":"2025-12-31T21:00:05.624504Z","iopub.status.idle":"2025-12-31T21:00:05.634536Z","shell.execute_reply.started":"2025-12-31T21:00:05.624487Z","shell.execute_reply":"2025-12-31T21:00:05.633811Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e12/sample_submission.csv\n/kaggle/input/playground-series-s5e12/train.csv\n/kaggle/input/playground-series-s5e12/test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ===============================================================\n# GPU AutoML - XGBoost, LightGBM, CatBoost\n# ===============================================================\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, mean_squared_error\n\n# ML Models\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\n\n# ------------------ Random seed ------------------\nRANDOM_STATE = 42\n\n# ------------------ User Inputs ------------------\ntrain_path = \"/kaggle/input/playground-series-s5e12/train.csv\"\ntest_path  = \"/kaggle/input/playground-series-s5e12/test.csv\"\ntarget_col = \"diagnosed_diabetes\"\ntask_type = \"classification\"  # 'regression' or 'classification'\n\n# ------------------ Load data ------------------\ntrain_df = pd.read_csv(train_path)\ntest_df  = pd.read_csv(test_path)\n\n# ------------------ Data cleaning ------------------\ndef clean_data(df):\n    df = df.copy()\n    for col in df.select_dtypes(include=np.number).columns:\n        df[col] = df[col].fillna(df[col].median())\n    for col in df.select_dtypes(include='object').columns:\n        df[col] = df[col].fillna(df[col].mode()[0])\n        df[col] = LabelEncoder().fit_transform(df[col])\n    return df\n\ntrain_df = clean_data(train_df)\ntest_df  = clean_data(test_df)\n\n# ------------------ Split features & target ------------------\nX_train = train_df.drop(columns=[target_col])\ny_train = train_df[target_col]\nX_test  = test_df.drop(columns=[target_col]) if target_col in test_df.columns else test_df\ny_test  = test_df[target_col] if target_col in test_df.columns else None\n\n# ------------------ Split validation ------------------\nX_train_sub, X_val, y_train_sub, y_val = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=RANDOM_STATE\n)\n\nprint(f\"Task Type: {task_type}, Train: {X_train_sub.shape}, Val: {X_val.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T21:00:08.412618Z","iopub.execute_input":"2025-12-31T21:00:08.412887Z","iopub.status.idle":"2025-12-31T21:00:19.159448Z","shell.execute_reply.started":"2025-12-31T21:00:08.412865Z","shell.execute_reply":"2025-12-31T21:00:19.158660Z"}},"outputs":[{"name":"stdout","text":"Task Type: classification, Train: (560000, 25), Val: (140000, 25)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ------------------ Initialize GPU models ------------------\nmodels = {}\n\nif task_type == 'regression':\n    models['XGBRegressor'] = XGBRegressor(booster='gbtree', tree_method='hist', eval_metric='rmse',\n                                         verbosity=0, enable_categorical=True, random_state=RANDOM_STATE, device='cuda', n_jobs=-1)\n    models['LGBMRegressor'] = LGBMRegressor(random_state=RANDOM_STATE, n_jobs=-1, device='gpu')\n    models['CatBoostRegressor'] = CatBoostRegressor(random_state=RANDOM_STATE, verbose=0, task_type='GPU', bootstrap_type='Bernoulli')\nelse:\n    models['XGBClassifier'] = XGBClassifier(booster='gbtree', tree_method='hist', eval_metric='auc',\n                                           verbosity=0, enable_categorical=True, random_state=RANDOM_STATE,\n                                           device='cuda', n_jobs=-1, early_stopping_rounds=100)\n    models['LGBMClassifier'] = LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1, device='gpu')\n    models['CatBoostClassifier'] = CatBoostClassifier(random_state=RANDOM_STATE, verbose=0, task_type='GPU', bootstrap_type='Bernoulli')\n\n# ------------------ Hyperparameter grids ------------------\nparam_grids = {\n    'XGBClassifier': {\n        'n_estimators': [800, 1500, 3000, 4500],\n        'learning_rate': [0.005, 0.01, 0.03],\n        'max_depth': [3, 5, 7],\n        'subsample': [0.7, 0.85, 0.95],\n        'colsample_bytree': [0.4, 0.6, 0.8],\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.0, 0.01, 0.1],\n        'reg_alpha': [0.0, 1e-4, 1e-2],\n        'reg_lambda': [1e-3, 1e-1, 1.0]\n    },\n    'LGBMClassifier': {\n        'n_estimators': [500, 1000, 2000, 5000],\n        'learning_rate': [0.005, 0.01, 0.05],\n        'num_leaves': [31, 63, 127],\n        'max_depth': [-1, 6, 10],\n        'subsample': [0.7, 0.85, 0.95],\n        'colsample_bytree': [0.6, 0.8, 1.0]\n    },\n    'CatBoostClassifier': {\n        'iterations': [800, 1500, 3000],\n        'learning_rate': [0.01, 0.03, 0.1],\n        'depth': [4, 6, 8],\n        'l2_leaf_reg': [1, 3, 5, 9],\n        'subsample': [0.7, 0.85, 1.0],\n        'border_count': [64, 128, 254],\n        'bootstrap_type': ['Bernoulli']  # Required for GPU\n    }\n}\n\n# ------------------ Helper to sample hyperparameters ------------------\nimport random\ndef sample_params(model_name, param_grid):\n    return {k: random.choice(v) for k, v in param_grid.get(model_name, {}).items()}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T21:00:22.369072Z","iopub.execute_input":"2025-12-31T21:00:22.369734Z","iopub.status.idle":"2025-12-31T21:00:22.380201Z","shell.execute_reply.started":"2025-12-31T21:00:22.369711Z","shell.execute_reply":"2025-12-31T21:00:22.379518Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"results = []\n\nfor name, base_model in models.items():\n    print(f\"\\nTraining {name} on GPU...\")\n\n    # Sample hyperparameters\n    sampled_params = sample_params(name, param_grids)\n\n    # Rebuild CatBoost with sampled params (cannot set after fit)\n    if name.startswith(\"CatBoost\"):\n        model = type(base_model)(**{**base_model.get_params(), **sampled_params})\n    else:\n        model = base_model\n        if sampled_params:\n            model.set_params(**sampled_params)\n\n    # -------------------- Train --------------------\n    if name.startswith(\"XGB\"):\n        model.fit(X_train_sub, y_train_sub, eval_set=[(X_val, y_val)], verbose=False)\n        y_pred_train = model.predict(X_train_sub)\n        y_pred_val = model.predict(X_val)\n\n        hyperparams_display = {\n            'n_estimators': model.best_iteration if hasattr(model, 'best_iteration') else model.n_estimators,\n            'learning_rate': model.learning_rate,\n            'max_depth': model.max_depth,\n            'subsample': model.subsample,\n            'colsample_bytree': model.colsample_bytree,\n            'min_child_weight': model.min_child_weight,\n            'gamma': model.gamma,\n            'reg_alpha': model.reg_alpha,\n            'reg_lambda': model.reg_lambda\n        }\n\n    elif name.startswith(\"LGBM\"):\n        model.fit(X_train_sub, y_train_sub)\n        y_pred_train = model.predict(X_train_sub)\n        y_pred_val = model.predict(X_val)\n\n        hyperparams_display = {\n            'n_estimators': model.n_estimators,\n            'learning_rate': model.learning_rate,\n            'num_leaves': model.num_leaves,\n            'max_depth': model.max_depth,\n            'subsample': model.subsample,\n            'colsample_bytree': model.colsample_bytree\n        }\n\n    elif name.startswith(\"CatBoost\"):\n        model.fit(X_train_sub, y_train_sub, eval_set=(X_val, y_val), use_best_model=True, verbose=False)\n        y_pred_train = model.predict(X_train_sub)\n        y_pred_val = model.predict(X_val)\n\n        params = model.get_params()\n        hyperparams_display = {\n            'iterations': model.tree_count_,\n            'learning_rate': params.get('learning_rate'),\n            'depth': params.get('depth'),\n            'l2_leaf_reg': params.get('l2_leaf_reg'),\n            'subsample': params.get('subsample'),\n            'border_count': params.get('border_count'),\n            'bootstrap_type': params.get('bootstrap_type'),\n            'best_iteration': model.get_best_iteration()\n        }\n\n    # -------------------- Scoring --------------------\n    if task_type == 'regression':\n        train_score = mean_squared_error(y_train_sub, y_pred_train, squared=False)\n        val_score = mean_squared_error(y_val, y_pred_val, squared=False)\n        overfit = \"Yes\" if train_score < val_score else \"No\"\n    else:\n        train_score = accuracy_score(y_train_sub, y_pred_train)\n        val_score = accuracy_score(y_val, y_pred_val)\n        overfit = \"Yes\" if train_score > val_score else \"No\"\n\n    results.append({\n        'Model': name,\n        'Train_Score': train_score,\n        'Validation_Score': val_score,\n        'Overfitting': overfit,\n        'Used_Params': hyperparams_display\n    })\n\n# -------------------- Display --------------------\nresults_df = pd.DataFrame(results).sort_values('Validation_Score', ascending=False if task_type=='classification' else True)\n\nfor _, row in results_df.iterrows():\n    print(f\"\\nüîπ {row['Model']}\")\n    print(f\"Train Score: {row['Train_Score']:.4f}\")\n    print(f\"Validation Score: {row['Validation_Score']:.4f}\")\n    print(f\"Overfitting: {row['Overfitting']}\")\n    print(\"Hyperparameters Used:\")\n    for k, v in row['Used_Params'].items():\n        print(f\"  - {k}: {v}\")\n\n# -------------------- Best Model --------------------\nbest_row = results_df.iloc[0] if task_type=='classification' else results_df.iloc[-1]\nprint(\"\\n\" + \"=\"*60)\nprint(\"üèÜ BEST MODEL OVERALL\")\nprint(\"=\"*60)\nprint(f\"Model Name       : {best_row['Model']}\")\nprint(f\"Train Score      : {best_row['Train_Score']:.6f}\")\nprint(f\"Validation Score : {best_row['Validation_Score']:.6f}\")\nprint(f\"Overfitting      : {best_row['Overfitting']}\")\nprint(\"\\nüîß Hyperparameters Used:\")\nfor k, v in best_row['Used_Params'].items():\n    print(f\"{k}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T21:00:25.260582Z","iopub.execute_input":"2025-12-31T21:00:25.260852Z","iopub.status.idle":"2025-12-31T21:02:49.489380Z","shell.execute_reply.started":"2025-12-31T21:00:25.260833Z","shell.execute_reply":"2025-12-31T21:02:49.488634Z"}},"outputs":[{"name":"stdout","text":"\nTraining XGBClassifier on GPU...\n\nTraining LGBMClassifier on GPU...\n[LightGBM] [Info] Number of positive: 348936, number of negative: 211064\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1900\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 25\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 20 dense feature groups (10.68 MB) transferred to GPU in 0.019400 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623100 -> initscore=0.502727\n[LightGBM] [Info] Start training from score 0.502727\n\nTraining CatBoostClassifier on GPU...\n\nüîπ LGBMClassifier\nTrain Score: 0.7123\nValidation Score: 0.6842\nOverfitting: Yes\nHyperparameters Used:\n  - n_estimators: 2000\n  - learning_rate: 0.01\n  - num_leaves: 127\n  - max_depth: -1\n  - subsample: 0.95\n  - colsample_bytree: 1.0\n\nüîπ XGBClassifier\nTrain Score: 0.6877\nValidation Score: 0.6804\nOverfitting: Yes\nHyperparameters Used:\n  - n_estimators: 799\n  - learning_rate: 0.01\n  - max_depth: 7\n  - subsample: 0.95\n  - colsample_bytree: 0.6\n  - min_child_weight: 1\n  - gamma: 0.01\n  - reg_alpha: 0.01\n  - reg_lambda: 1.0\n\nüîπ CatBoostClassifier\nTrain Score: 0.6795\nValidation Score: 0.6757\nOverfitting: Yes\nHyperparameters Used:\n  - iterations: 792\n  - learning_rate: 0.1\n  - depth: 4\n  - l2_leaf_reg: 9\n  - subsample: 1.0\n  - border_count: 64\n  - bootstrap_type: Bernoulli\n  - best_iteration: 791\n\n============================================================\nüèÜ BEST MODEL OVERALL\n============================================================\nModel Name       : LGBMClassifier\nTrain Score      : 0.712311\nValidation Score : 0.684243\nOverfitting      : Yes\n\nüîß Hyperparameters Used:\nn_estimators: 2000\nlearning_rate: 0.01\nnum_leaves: 127\nmax_depth: -1\nsubsample: 0.95\ncolsample_bytree: 1.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\n# -------------------- Flatten results --------------------\nflattened_results = []\nfor r in results:\n    flat_dict = r.copy()\n    hyperparams = flat_dict.pop('Used_Params')\n    for k, v in hyperparams.items():\n        flat_dict[k] = v\n    flattened_results.append(flat_dict)\n\n# Create DataFrame\nresults_df = pd.DataFrame(flattened_results)\n\n# Sort by Validation Score\nresults_df = results_df.sort_values(\n    'Validation_Score', \n    ascending=False if task_type=='classification' else True\n).reset_index(drop=True)\n\n# Mark best model\nbest_index = 0 if task_type=='classification' else results_df['Validation_Score'].idxmin()\nresults_df['Best_Model üèÜ'] = ''\nresults_df.loc[best_index, 'Best_Model üèÜ'] = 'üèÜ'\n\n# -------------------- Styling --------------------\ndef highlight_models(row):\n    if row['Best_Model üèÜ'] == 'üèÜ':\n        return ['background-color: #b6fcd5'] * len(row)  # Light green for best model\n    elif row['Overfitting'] == 'Yes':\n        return ['background-color: #fcb6b6'] * len(row)  # Light red for overfitting\n    else:\n        return [''] * len(row)  # No highlight\n\n# Display styled DataFrame\npd.set_option('display.max_columns', None)\nstyled_df = results_df.style.apply(highlight_models, axis=1)\ndisplay(styled_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T21:15:50.499498Z","iopub.execute_input":"2025-12-31T21:15:50.500008Z","iopub.status.idle":"2025-12-31T21:15:50.578163Z","shell.execute_reply.started":"2025-12-31T21:15:50.499983Z","shell.execute_reply":"2025-12-31T21:15:50.577348Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7ab422370c50>","text/html":"<style type=\"text/css\">\n#T_e2b89_row0_col0, #T_e2b89_row0_col1, #T_e2b89_row0_col2, #T_e2b89_row0_col3, #T_e2b89_row0_col4, #T_e2b89_row0_col5, #T_e2b89_row0_col6, #T_e2b89_row0_col7, #T_e2b89_row0_col8, #T_e2b89_row0_col9, #T_e2b89_row0_col10, #T_e2b89_row0_col11, #T_e2b89_row0_col12, #T_e2b89_row0_col13, #T_e2b89_row0_col14, #T_e2b89_row0_col15, #T_e2b89_row0_col16, #T_e2b89_row0_col17, #T_e2b89_row0_col18, #T_e2b89_row0_col19, #T_e2b89_row0_col20 {\n  background-color: #b6fcd5;\n}\n#T_e2b89_row1_col0, #T_e2b89_row1_col1, #T_e2b89_row1_col2, #T_e2b89_row1_col3, #T_e2b89_row1_col4, #T_e2b89_row1_col5, #T_e2b89_row1_col6, #T_e2b89_row1_col7, #T_e2b89_row1_col8, #T_e2b89_row1_col9, #T_e2b89_row1_col10, #T_e2b89_row1_col11, #T_e2b89_row1_col12, #T_e2b89_row1_col13, #T_e2b89_row1_col14, #T_e2b89_row1_col15, #T_e2b89_row1_col16, #T_e2b89_row1_col17, #T_e2b89_row1_col18, #T_e2b89_row1_col19, #T_e2b89_row1_col20, #T_e2b89_row2_col0, #T_e2b89_row2_col1, #T_e2b89_row2_col2, #T_e2b89_row2_col3, #T_e2b89_row2_col4, #T_e2b89_row2_col5, #T_e2b89_row2_col6, #T_e2b89_row2_col7, #T_e2b89_row2_col8, #T_e2b89_row2_col9, #T_e2b89_row2_col10, #T_e2b89_row2_col11, #T_e2b89_row2_col12, #T_e2b89_row2_col13, #T_e2b89_row2_col14, #T_e2b89_row2_col15, #T_e2b89_row2_col16, #T_e2b89_row2_col17, #T_e2b89_row2_col18, #T_e2b89_row2_col19, #T_e2b89_row2_col20 {\n  background-color: #fcb6b6;\n}\n</style>\n<table id=\"T_e2b89\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_e2b89_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n      <th id=\"T_e2b89_level0_col1\" class=\"col_heading level0 col1\" >Train_Score</th>\n      <th id=\"T_e2b89_level0_col2\" class=\"col_heading level0 col2\" >Validation_Score</th>\n      <th id=\"T_e2b89_level0_col3\" class=\"col_heading level0 col3\" >Overfitting</th>\n      <th id=\"T_e2b89_level0_col4\" class=\"col_heading level0 col4\" >n_estimators</th>\n      <th id=\"T_e2b89_level0_col5\" class=\"col_heading level0 col5\" >learning_rate</th>\n      <th id=\"T_e2b89_level0_col6\" class=\"col_heading level0 col6\" >max_depth</th>\n      <th id=\"T_e2b89_level0_col7\" class=\"col_heading level0 col7\" >subsample</th>\n      <th id=\"T_e2b89_level0_col8\" class=\"col_heading level0 col8\" >colsample_bytree</th>\n      <th id=\"T_e2b89_level0_col9\" class=\"col_heading level0 col9\" >min_child_weight</th>\n      <th id=\"T_e2b89_level0_col10\" class=\"col_heading level0 col10\" >gamma</th>\n      <th id=\"T_e2b89_level0_col11\" class=\"col_heading level0 col11\" >reg_alpha</th>\n      <th id=\"T_e2b89_level0_col12\" class=\"col_heading level0 col12\" >reg_lambda</th>\n      <th id=\"T_e2b89_level0_col13\" class=\"col_heading level0 col13\" >num_leaves</th>\n      <th id=\"T_e2b89_level0_col14\" class=\"col_heading level0 col14\" >iterations</th>\n      <th id=\"T_e2b89_level0_col15\" class=\"col_heading level0 col15\" >depth</th>\n      <th id=\"T_e2b89_level0_col16\" class=\"col_heading level0 col16\" >l2_leaf_reg</th>\n      <th id=\"T_e2b89_level0_col17\" class=\"col_heading level0 col17\" >border_count</th>\n      <th id=\"T_e2b89_level0_col18\" class=\"col_heading level0 col18\" >bootstrap_type</th>\n      <th id=\"T_e2b89_level0_col19\" class=\"col_heading level0 col19\" >best_iteration</th>\n      <th id=\"T_e2b89_level0_col20\" class=\"col_heading level0 col20\" >Best_Model üèÜ</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_e2b89_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_e2b89_row0_col0\" class=\"data row0 col0\" >LGBMClassifier</td>\n      <td id=\"T_e2b89_row0_col1\" class=\"data row0 col1\" >0.712311</td>\n      <td id=\"T_e2b89_row0_col2\" class=\"data row0 col2\" >0.684243</td>\n      <td id=\"T_e2b89_row0_col3\" class=\"data row0 col3\" >Yes</td>\n      <td id=\"T_e2b89_row0_col4\" class=\"data row0 col4\" >2000.000000</td>\n      <td id=\"T_e2b89_row0_col5\" class=\"data row0 col5\" >0.010000</td>\n      <td id=\"T_e2b89_row0_col6\" class=\"data row0 col6\" >-1.000000</td>\n      <td id=\"T_e2b89_row0_col7\" class=\"data row0 col7\" >0.950000</td>\n      <td id=\"T_e2b89_row0_col8\" class=\"data row0 col8\" >1.000000</td>\n      <td id=\"T_e2b89_row0_col9\" class=\"data row0 col9\" >nan</td>\n      <td id=\"T_e2b89_row0_col10\" class=\"data row0 col10\" >nan</td>\n      <td id=\"T_e2b89_row0_col11\" class=\"data row0 col11\" >nan</td>\n      <td id=\"T_e2b89_row0_col12\" class=\"data row0 col12\" >nan</td>\n      <td id=\"T_e2b89_row0_col13\" class=\"data row0 col13\" >127.000000</td>\n      <td id=\"T_e2b89_row0_col14\" class=\"data row0 col14\" >nan</td>\n      <td id=\"T_e2b89_row0_col15\" class=\"data row0 col15\" >nan</td>\n      <td id=\"T_e2b89_row0_col16\" class=\"data row0 col16\" >nan</td>\n      <td id=\"T_e2b89_row0_col17\" class=\"data row0 col17\" >nan</td>\n      <td id=\"T_e2b89_row0_col18\" class=\"data row0 col18\" >nan</td>\n      <td id=\"T_e2b89_row0_col19\" class=\"data row0 col19\" >nan</td>\n      <td id=\"T_e2b89_row0_col20\" class=\"data row0 col20\" >üèÜ</td>\n    </tr>\n    <tr>\n      <th id=\"T_e2b89_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_e2b89_row1_col0\" class=\"data row1 col0\" >XGBClassifier</td>\n      <td id=\"T_e2b89_row1_col1\" class=\"data row1 col1\" >0.687661</td>\n      <td id=\"T_e2b89_row1_col2\" class=\"data row1 col2\" >0.680436</td>\n      <td id=\"T_e2b89_row1_col3\" class=\"data row1 col3\" >Yes</td>\n      <td id=\"T_e2b89_row1_col4\" class=\"data row1 col4\" >799.000000</td>\n      <td id=\"T_e2b89_row1_col5\" class=\"data row1 col5\" >0.010000</td>\n      <td id=\"T_e2b89_row1_col6\" class=\"data row1 col6\" >7.000000</td>\n      <td id=\"T_e2b89_row1_col7\" class=\"data row1 col7\" >0.950000</td>\n      <td id=\"T_e2b89_row1_col8\" class=\"data row1 col8\" >0.600000</td>\n      <td id=\"T_e2b89_row1_col9\" class=\"data row1 col9\" >1.000000</td>\n      <td id=\"T_e2b89_row1_col10\" class=\"data row1 col10\" >0.010000</td>\n      <td id=\"T_e2b89_row1_col11\" class=\"data row1 col11\" >0.010000</td>\n      <td id=\"T_e2b89_row1_col12\" class=\"data row1 col12\" >1.000000</td>\n      <td id=\"T_e2b89_row1_col13\" class=\"data row1 col13\" >nan</td>\n      <td id=\"T_e2b89_row1_col14\" class=\"data row1 col14\" >nan</td>\n      <td id=\"T_e2b89_row1_col15\" class=\"data row1 col15\" >nan</td>\n      <td id=\"T_e2b89_row1_col16\" class=\"data row1 col16\" >nan</td>\n      <td id=\"T_e2b89_row1_col17\" class=\"data row1 col17\" >nan</td>\n      <td id=\"T_e2b89_row1_col18\" class=\"data row1 col18\" >nan</td>\n      <td id=\"T_e2b89_row1_col19\" class=\"data row1 col19\" >nan</td>\n      <td id=\"T_e2b89_row1_col20\" class=\"data row1 col20\" ></td>\n    </tr>\n    <tr>\n      <th id=\"T_e2b89_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_e2b89_row2_col0\" class=\"data row2 col0\" >CatBoostClassifier</td>\n      <td id=\"T_e2b89_row2_col1\" class=\"data row2 col1\" >0.679461</td>\n      <td id=\"T_e2b89_row2_col2\" class=\"data row2 col2\" >0.675743</td>\n      <td id=\"T_e2b89_row2_col3\" class=\"data row2 col3\" >Yes</td>\n      <td id=\"T_e2b89_row2_col4\" class=\"data row2 col4\" >nan</td>\n      <td id=\"T_e2b89_row2_col5\" class=\"data row2 col5\" >0.100000</td>\n      <td id=\"T_e2b89_row2_col6\" class=\"data row2 col6\" >nan</td>\n      <td id=\"T_e2b89_row2_col7\" class=\"data row2 col7\" >1.000000</td>\n      <td id=\"T_e2b89_row2_col8\" class=\"data row2 col8\" >nan</td>\n      <td id=\"T_e2b89_row2_col9\" class=\"data row2 col9\" >nan</td>\n      <td id=\"T_e2b89_row2_col10\" class=\"data row2 col10\" >nan</td>\n      <td id=\"T_e2b89_row2_col11\" class=\"data row2 col11\" >nan</td>\n      <td id=\"T_e2b89_row2_col12\" class=\"data row2 col12\" >nan</td>\n      <td id=\"T_e2b89_row2_col13\" class=\"data row2 col13\" >nan</td>\n      <td id=\"T_e2b89_row2_col14\" class=\"data row2 col14\" >792.000000</td>\n      <td id=\"T_e2b89_row2_col15\" class=\"data row2 col15\" >4.000000</td>\n      <td id=\"T_e2b89_row2_col16\" class=\"data row2 col16\" >9.000000</td>\n      <td id=\"T_e2b89_row2_col17\" class=\"data row2 col17\" >64.000000</td>\n      <td id=\"T_e2b89_row2_col18\" class=\"data row2 col18\" >Bernoulli</td>\n      <td id=\"T_e2b89_row2_col19\" class=\"data row2 col19\" >791.000000</td>\n      <td id=\"T_e2b89_row2_col20\" class=\"data row2 col20\" ></td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# -------------------- Flatten results --------------------\nflattened_results = []\nfor r in results:\n    flat_dict = r.copy()\n    hyperparams = flat_dict.pop('Used_Params')\n    for k, v in hyperparams.items():\n        flat_dict[k] = v\n    flattened_results.append(flat_dict)\n\n# Create DataFrame\nresults_df = pd.DataFrame(flattened_results)\n\n# -------------------- Compute Gap (Train - Val) --------------------\nresults_df['Gap (Train - Val)'] = abs(results_df['Train_Score'] - results_df['Validation_Score']).round(4)\n\n# -------------------- Add Star Ranking based on Gap --------------------\n# Smaller gap = better (fewer stars), larger gap = more stars\nn = len(results_df)\n# Rank by Gap ascending (smallest = rank 1)\nresults_df['Gap_Rank'] = results_df['Gap (Train - Val)'].rank(method='min').astype(int)\n\n# Function to convert rank to stars\ndef gap_to_stars(rank, n):\n    if rank <= n/3:\n        return '‚≠ê'           # least overfit\n    elif rank <= 2*n/3:\n        return '‚≠ê‚≠ê'\n    else:\n        return '‚≠ê‚≠ê‚≠ê'         # most overfit\n\nresults_df['Gap_Stars'] = results_df['Gap_Rank'].apply(lambda x: gap_to_stars(x, n))\n\n# -------------------- Compute Overfitting Score --------------------\nif task_type == 'classification':\n    results_df['Overfit_Score'] = results_df['Train_Score'] - results_df['Validation_Score']\n    results_df['Sort_Validation'] = results_df['Validation_Score']  # Higher is better\n    results_df['Sort_Overfit'] = -results_df['Overfit_Score']\nelse:\n    results_df['Overfit_Score'] = results_df['Validation_Score'] - results_df['Train_Score']\n    results_df['Sort_Validation'] = -results_df['Validation_Score']  # Lower is better\n    results_df['Sort_Overfit'] = results_df['Overfit_Score']\n\n# -------------------- Rank Overfitting --------------------\nresults_df['Overfit_Rank'] = results_df['Overfit_Score'].rank(method='min', ascending=False).astype(int)\n\n# -------------------- Combined Rank --------------------\nresults_df = results_df.sort_values(['Sort_Validation', 'Sort_Overfit'], ascending=True).reset_index(drop=True)\nresults_df['Combined_Rank'] = range(1, len(results_df)+1)\n\n# -------------------- Mark Best Model üèÜ --------------------\nbest_index = 0 if task_type=='classification' else results_df['Validation_Score'].idxmax()\nresults_df['Best_Model üèÜ'] = ''\nresults_df.loc[best_index, 'Best_Model üèÜ'] = 'üèÜ'\n\n# -------------------- Styling --------------------\ndef highlight_models(row):\n    if row['Best_Model üèÜ'] == 'üèÜ':\n        return ['background-color: #b6fcd5'] * len(row)  # Green for best\n    elif row['Overfit_Score'] > 0:\n        return ['background-color: #fcb6b6'] * len(row)  # Red for overfitting\n    else:\n        return [''] * len(row)\n\npd.set_option('display.max_columns', None)\nstyled_df = results_df.style.apply(highlight_models, axis=1)\ndisplay(styled_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T21:27:29.445806Z","iopub.execute_input":"2025-12-31T21:27:29.446477Z","iopub.status.idle":"2025-12-31T21:27:29.472637Z","shell.execute_reply.started":"2025-12-31T21:27:29.446451Z","shell.execute_reply":"2025-12-31T21:27:29.472011Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7ab422f2dbd0>","text/html":"<style type=\"text/css\">\n#T_666c0_row0_col0, #T_666c0_row0_col1, #T_666c0_row0_col2, #T_666c0_row0_col3, #T_666c0_row0_col4, #T_666c0_row0_col5, #T_666c0_row0_col6, #T_666c0_row0_col7, #T_666c0_row0_col8, #T_666c0_row0_col9, #T_666c0_row0_col10, #T_666c0_row0_col11, #T_666c0_row0_col12, #T_666c0_row0_col13, #T_666c0_row0_col14, #T_666c0_row0_col15, #T_666c0_row0_col16, #T_666c0_row0_col17, #T_666c0_row0_col18, #T_666c0_row0_col19, #T_666c0_row0_col20, #T_666c0_row0_col21, #T_666c0_row0_col22, #T_666c0_row0_col23, #T_666c0_row0_col24, #T_666c0_row0_col25, #T_666c0_row0_col26, #T_666c0_row0_col27, #T_666c0_row0_col28 {\n  background-color: #b6fcd5;\n}\n#T_666c0_row1_col0, #T_666c0_row1_col1, #T_666c0_row1_col2, #T_666c0_row1_col3, #T_666c0_row1_col4, #T_666c0_row1_col5, #T_666c0_row1_col6, #T_666c0_row1_col7, #T_666c0_row1_col8, #T_666c0_row1_col9, #T_666c0_row1_col10, #T_666c0_row1_col11, #T_666c0_row1_col12, #T_666c0_row1_col13, #T_666c0_row1_col14, #T_666c0_row1_col15, #T_666c0_row1_col16, #T_666c0_row1_col17, #T_666c0_row1_col18, #T_666c0_row1_col19, #T_666c0_row1_col20, #T_666c0_row1_col21, #T_666c0_row1_col22, #T_666c0_row1_col23, #T_666c0_row1_col24, #T_666c0_row1_col25, #T_666c0_row1_col26, #T_666c0_row1_col27, #T_666c0_row1_col28, #T_666c0_row2_col0, #T_666c0_row2_col1, #T_666c0_row2_col2, #T_666c0_row2_col3, #T_666c0_row2_col4, #T_666c0_row2_col5, #T_666c0_row2_col6, #T_666c0_row2_col7, #T_666c0_row2_col8, #T_666c0_row2_col9, #T_666c0_row2_col10, #T_666c0_row2_col11, #T_666c0_row2_col12, #T_666c0_row2_col13, #T_666c0_row2_col14, #T_666c0_row2_col15, #T_666c0_row2_col16, #T_666c0_row2_col17, #T_666c0_row2_col18, #T_666c0_row2_col19, #T_666c0_row2_col20, #T_666c0_row2_col21, #T_666c0_row2_col22, #T_666c0_row2_col23, #T_666c0_row2_col24, #T_666c0_row2_col25, #T_666c0_row2_col26, #T_666c0_row2_col27, #T_666c0_row2_col28 {\n  background-color: #fcb6b6;\n}\n</style>\n<table id=\"T_666c0\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_666c0_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n      <th id=\"T_666c0_level0_col1\" class=\"col_heading level0 col1\" >Train_Score</th>\n      <th id=\"T_666c0_level0_col2\" class=\"col_heading level0 col2\" >Validation_Score</th>\n      <th id=\"T_666c0_level0_col3\" class=\"col_heading level0 col3\" >Overfitting</th>\n      <th id=\"T_666c0_level0_col4\" class=\"col_heading level0 col4\" >n_estimators</th>\n      <th id=\"T_666c0_level0_col5\" class=\"col_heading level0 col5\" >learning_rate</th>\n      <th id=\"T_666c0_level0_col6\" class=\"col_heading level0 col6\" >max_depth</th>\n      <th id=\"T_666c0_level0_col7\" class=\"col_heading level0 col7\" >subsample</th>\n      <th id=\"T_666c0_level0_col8\" class=\"col_heading level0 col8\" >colsample_bytree</th>\n      <th id=\"T_666c0_level0_col9\" class=\"col_heading level0 col9\" >min_child_weight</th>\n      <th id=\"T_666c0_level0_col10\" class=\"col_heading level0 col10\" >gamma</th>\n      <th id=\"T_666c0_level0_col11\" class=\"col_heading level0 col11\" >reg_alpha</th>\n      <th id=\"T_666c0_level0_col12\" class=\"col_heading level0 col12\" >reg_lambda</th>\n      <th id=\"T_666c0_level0_col13\" class=\"col_heading level0 col13\" >num_leaves</th>\n      <th id=\"T_666c0_level0_col14\" class=\"col_heading level0 col14\" >iterations</th>\n      <th id=\"T_666c0_level0_col15\" class=\"col_heading level0 col15\" >depth</th>\n      <th id=\"T_666c0_level0_col16\" class=\"col_heading level0 col16\" >l2_leaf_reg</th>\n      <th id=\"T_666c0_level0_col17\" class=\"col_heading level0 col17\" >border_count</th>\n      <th id=\"T_666c0_level0_col18\" class=\"col_heading level0 col18\" >bootstrap_type</th>\n      <th id=\"T_666c0_level0_col19\" class=\"col_heading level0 col19\" >best_iteration</th>\n      <th id=\"T_666c0_level0_col20\" class=\"col_heading level0 col20\" >Gap (Train - Val)</th>\n      <th id=\"T_666c0_level0_col21\" class=\"col_heading level0 col21\" >Gap_Rank</th>\n      <th id=\"T_666c0_level0_col22\" class=\"col_heading level0 col22\" >Gap_Stars</th>\n      <th id=\"T_666c0_level0_col23\" class=\"col_heading level0 col23\" >Overfit_Score</th>\n      <th id=\"T_666c0_level0_col24\" class=\"col_heading level0 col24\" >Sort_Validation</th>\n      <th id=\"T_666c0_level0_col25\" class=\"col_heading level0 col25\" >Sort_Overfit</th>\n      <th id=\"T_666c0_level0_col26\" class=\"col_heading level0 col26\" >Overfit_Rank</th>\n      <th id=\"T_666c0_level0_col27\" class=\"col_heading level0 col27\" >Combined_Rank</th>\n      <th id=\"T_666c0_level0_col28\" class=\"col_heading level0 col28\" >Best_Model üèÜ</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_666c0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_666c0_row0_col0\" class=\"data row0 col0\" >CatBoostClassifier</td>\n      <td id=\"T_666c0_row0_col1\" class=\"data row0 col1\" >0.679461</td>\n      <td id=\"T_666c0_row0_col2\" class=\"data row0 col2\" >0.675743</td>\n      <td id=\"T_666c0_row0_col3\" class=\"data row0 col3\" >Yes</td>\n      <td id=\"T_666c0_row0_col4\" class=\"data row0 col4\" >nan</td>\n      <td id=\"T_666c0_row0_col5\" class=\"data row0 col5\" >0.100000</td>\n      <td id=\"T_666c0_row0_col6\" class=\"data row0 col6\" >nan</td>\n      <td id=\"T_666c0_row0_col7\" class=\"data row0 col7\" >1.000000</td>\n      <td id=\"T_666c0_row0_col8\" class=\"data row0 col8\" >nan</td>\n      <td id=\"T_666c0_row0_col9\" class=\"data row0 col9\" >nan</td>\n      <td id=\"T_666c0_row0_col10\" class=\"data row0 col10\" >nan</td>\n      <td id=\"T_666c0_row0_col11\" class=\"data row0 col11\" >nan</td>\n      <td id=\"T_666c0_row0_col12\" class=\"data row0 col12\" >nan</td>\n      <td id=\"T_666c0_row0_col13\" class=\"data row0 col13\" >nan</td>\n      <td id=\"T_666c0_row0_col14\" class=\"data row0 col14\" >792.000000</td>\n      <td id=\"T_666c0_row0_col15\" class=\"data row0 col15\" >4.000000</td>\n      <td id=\"T_666c0_row0_col16\" class=\"data row0 col16\" >9.000000</td>\n      <td id=\"T_666c0_row0_col17\" class=\"data row0 col17\" >64.000000</td>\n      <td id=\"T_666c0_row0_col18\" class=\"data row0 col18\" >Bernoulli</td>\n      <td id=\"T_666c0_row0_col19\" class=\"data row0 col19\" >791.000000</td>\n      <td id=\"T_666c0_row0_col20\" class=\"data row0 col20\" >0.003700</td>\n      <td id=\"T_666c0_row0_col21\" class=\"data row0 col21\" >1</td>\n      <td id=\"T_666c0_row0_col22\" class=\"data row0 col22\" >‚≠ê</td>\n      <td id=\"T_666c0_row0_col23\" class=\"data row0 col23\" >0.003718</td>\n      <td id=\"T_666c0_row0_col24\" class=\"data row0 col24\" >0.675743</td>\n      <td id=\"T_666c0_row0_col25\" class=\"data row0 col25\" >-0.003718</td>\n      <td id=\"T_666c0_row0_col26\" class=\"data row0 col26\" >3</td>\n      <td id=\"T_666c0_row0_col27\" class=\"data row0 col27\" >1</td>\n      <td id=\"T_666c0_row0_col28\" class=\"data row0 col28\" >üèÜ</td>\n    </tr>\n    <tr>\n      <th id=\"T_666c0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_666c0_row1_col0\" class=\"data row1 col0\" >XGBClassifier</td>\n      <td id=\"T_666c0_row1_col1\" class=\"data row1 col1\" >0.687661</td>\n      <td id=\"T_666c0_row1_col2\" class=\"data row1 col2\" >0.680436</td>\n      <td id=\"T_666c0_row1_col3\" class=\"data row1 col3\" >Yes</td>\n      <td id=\"T_666c0_row1_col4\" class=\"data row1 col4\" >799.000000</td>\n      <td id=\"T_666c0_row1_col5\" class=\"data row1 col5\" >0.010000</td>\n      <td id=\"T_666c0_row1_col6\" class=\"data row1 col6\" >7.000000</td>\n      <td id=\"T_666c0_row1_col7\" class=\"data row1 col7\" >0.950000</td>\n      <td id=\"T_666c0_row1_col8\" class=\"data row1 col8\" >0.600000</td>\n      <td id=\"T_666c0_row1_col9\" class=\"data row1 col9\" >1.000000</td>\n      <td id=\"T_666c0_row1_col10\" class=\"data row1 col10\" >0.010000</td>\n      <td id=\"T_666c0_row1_col11\" class=\"data row1 col11\" >0.010000</td>\n      <td id=\"T_666c0_row1_col12\" class=\"data row1 col12\" >1.000000</td>\n      <td id=\"T_666c0_row1_col13\" class=\"data row1 col13\" >nan</td>\n      <td id=\"T_666c0_row1_col14\" class=\"data row1 col14\" >nan</td>\n      <td id=\"T_666c0_row1_col15\" class=\"data row1 col15\" >nan</td>\n      <td id=\"T_666c0_row1_col16\" class=\"data row1 col16\" >nan</td>\n      <td id=\"T_666c0_row1_col17\" class=\"data row1 col17\" >nan</td>\n      <td id=\"T_666c0_row1_col18\" class=\"data row1 col18\" >nan</td>\n      <td id=\"T_666c0_row1_col19\" class=\"data row1 col19\" >nan</td>\n      <td id=\"T_666c0_row1_col20\" class=\"data row1 col20\" >0.007200</td>\n      <td id=\"T_666c0_row1_col21\" class=\"data row1 col21\" >2</td>\n      <td id=\"T_666c0_row1_col22\" class=\"data row1 col22\" >‚≠ê‚≠ê</td>\n      <td id=\"T_666c0_row1_col23\" class=\"data row1 col23\" >0.007225</td>\n      <td id=\"T_666c0_row1_col24\" class=\"data row1 col24\" >0.680436</td>\n      <td id=\"T_666c0_row1_col25\" class=\"data row1 col25\" >-0.007225</td>\n      <td id=\"T_666c0_row1_col26\" class=\"data row1 col26\" >2</td>\n      <td id=\"T_666c0_row1_col27\" class=\"data row1 col27\" >2</td>\n      <td id=\"T_666c0_row1_col28\" class=\"data row1 col28\" ></td>\n    </tr>\n    <tr>\n      <th id=\"T_666c0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_666c0_row2_col0\" class=\"data row2 col0\" >LGBMClassifier</td>\n      <td id=\"T_666c0_row2_col1\" class=\"data row2 col1\" >0.712311</td>\n      <td id=\"T_666c0_row2_col2\" class=\"data row2 col2\" >0.684243</td>\n      <td id=\"T_666c0_row2_col3\" class=\"data row2 col3\" >Yes</td>\n      <td id=\"T_666c0_row2_col4\" class=\"data row2 col4\" >2000.000000</td>\n      <td id=\"T_666c0_row2_col5\" class=\"data row2 col5\" >0.010000</td>\n      <td id=\"T_666c0_row2_col6\" class=\"data row2 col6\" >-1.000000</td>\n      <td id=\"T_666c0_row2_col7\" class=\"data row2 col7\" >0.950000</td>\n      <td id=\"T_666c0_row2_col8\" class=\"data row2 col8\" >1.000000</td>\n      <td id=\"T_666c0_row2_col9\" class=\"data row2 col9\" >nan</td>\n      <td id=\"T_666c0_row2_col10\" class=\"data row2 col10\" >nan</td>\n      <td id=\"T_666c0_row2_col11\" class=\"data row2 col11\" >nan</td>\n      <td id=\"T_666c0_row2_col12\" class=\"data row2 col12\" >nan</td>\n      <td id=\"T_666c0_row2_col13\" class=\"data row2 col13\" >127.000000</td>\n      <td id=\"T_666c0_row2_col14\" class=\"data row2 col14\" >nan</td>\n      <td id=\"T_666c0_row2_col15\" class=\"data row2 col15\" >nan</td>\n      <td id=\"T_666c0_row2_col16\" class=\"data row2 col16\" >nan</td>\n      <td id=\"T_666c0_row2_col17\" class=\"data row2 col17\" >nan</td>\n      <td id=\"T_666c0_row2_col18\" class=\"data row2 col18\" >nan</td>\n      <td id=\"T_666c0_row2_col19\" class=\"data row2 col19\" >nan</td>\n      <td id=\"T_666c0_row2_col20\" class=\"data row2 col20\" >0.028100</td>\n      <td id=\"T_666c0_row2_col21\" class=\"data row2 col21\" >3</td>\n      <td id=\"T_666c0_row2_col22\" class=\"data row2 col22\" >‚≠ê‚≠ê‚≠ê</td>\n      <td id=\"T_666c0_row2_col23\" class=\"data row2 col23\" >0.028068</td>\n      <td id=\"T_666c0_row2_col24\" class=\"data row2 col24\" >0.684243</td>\n      <td id=\"T_666c0_row2_col25\" class=\"data row2 col25\" >-0.028068</td>\n      <td id=\"T_666c0_row2_col26\" class=\"data row2 col26\" >1</td>\n      <td id=\"T_666c0_row2_col27\" class=\"data row2 col27\" >3</td>\n      <td id=\"T_666c0_row2_col28\" class=\"data row2 col28\" ></td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":16}]}